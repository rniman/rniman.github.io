---
title: 파이썬과 케라스로 배우는 강화학습 | 강화학습 기초 3
date: 2025-09-13 15:35:33 +0900
categories: [Study, Study\ReinforceLearning]
tags: [AI, Reinforce Learning, Python, Book Note]
author: "rniman"                            # for single entry
# or
# authors: [<author1_id>, <author2_id>]   # for multiple entries
description: "파이썬과 케라스로 배우는 강화학습 Chapter 2 정리"
math: true
comments: false # 댓글 기능
---

## 책 정보 📖 

- 책 제목: 파이썬과 케라스로 배우는 강화학습
- 글쓴이: 이웅원, 양혁렬, 김건우, 이영무, 이의령
- 출판사: 위키북스
- 발행일: 2020년 04월 07일
- 챕터: Chapter 2. 강화학습 기초

### 책소개
강화학습의 기초부터 최근 알고리즘까지 친절하게 설명합니다!
‘알파고’로부터 받은 신선한 충격으로 많은 사람들이 강화학습에 관심을 가지기 시작했다. 하지만 처음 강화학습을 공부하는 분들을 위한 쉬운 자료나 강의를 찾아보기 어려웠다. 외국 강의를 통해 어렵게 이론을 공부하더라도 강화학습을 구현하는 데는 또 다른 장벽이 있었다. 이 책은 강화학습을 처음 공부하는 데 어려움을 겪는 독자를 위해 이론부터 코드 구현까지의 가이드를 제시한다.

특히 이번 개정판에서는 텐서플로 버전업에 맞춰서 코드를 업데이트하고 전반적인 이론 및 코드 설명을 개선했다. 그리고 실무에서 많이 활용될 수 있는 연속적 액터-크리틱 알고리즘을 추가했다.

* 출처 : [파이썬과 케라스로 배우는 강화학습 - 교보문고](https://product.kyobobook.co.kr/detail/S000001766424)

### 주요 내용
- 강화학습의 기초 - 벨만 방정식

    강화학습의 핵심인 벨만 방정식에 대해 알아본다. 벨만 방정식은 현재 상태의 가치함수와 다음 상태의 가치함수 사이의 관계를 나타내는 중요한 수학적 도구다.

---

## 벨만 기대 방정식

### 가치함수와 정책의 관계

가치함수는 어떤 상태의 가치를 나타낸다. 가치함수는 현재 에이전트 정책의 영향을 받는다.

$v_{\pi}(s) = E_{\pi} [ R_{t+1} + \gamma v_{\pi}(S_{t+1}) \| S_t = s]$

이것이 바로 **벨만 기대 방정식**이다. 기댓값의 개념이 들어가며, 기댓값을 알아내려면 앞으로 받을 모든 보상에 대해 고려해야 한다.

### 계산의 효율성 문제

정의상으로는 가능하지만 상태가 많아질수록 비효율적인 방법이다. 컴퓨터 계산에서 방정식을 풀 때 식 하나로 풀어내는 방법보다는 식 자체로는 풀리지 않지만 계속 계산을 하면서 푸는 방법을 사용한다.

예를 들어 1 + 1 + … + 1 = 100을 계산할 때의 한번의 식으로 나타내는 대신, 계속 더해나가 해결하는 방법을 사용한다:

```python
x = 0
for i in range(100):
    x += 1
```

이것이 바로 벨만 방정식을 이용해 가치함수를 계산하는 방식이다. 한번에 모든 것을 계산하는 것이 아닌 값을 변수에 저장하고 루프를 도는 계산을 통해 참 값을 알아나가는 것으로 다이내믹 프로그래밍과 연관된다.

코드에서 '=' 연산자는 대입하는 연산자다. 이를 이용해 가치함수 $v_{\pi}(s)$를 업데이트해나간다.

### 기댓값 계산 방법

업데이트를 하기 위해서는 기댓값을 계산해야 하는데 어떻게 할까?

기댓값에는 어떠한 행동을 할 확률(정책)과 그 행동을 했을 때 어떤 상태로 가게 될 확률(상태 변환 확률)이 포함되어 있다. 따라서 정책과 상태변환 확률을 포함해서 계산한다.

$v_{\pi}(s) = \sum\limits_{a\in A}\pi(a \| s)(r_{(s, a)} + \gamma\sum\limits_{s^{\prime}\in S}P_{ss^{\prime}}^a v_{\pi}(s^{\prime}))$

이것이 **계산 가능한 벨만 방정식**이다.

### 상태 변환 확률이 1인 경우

상태 변환 확률은 환경의 일부로 환경을 만드는 사람이 설정할 수 있다. 그리드에서 상태 변환 확률을 왼쪽으로 가는 행동을 할 때 1로 설정했다면 에이전트가 왼쪽으로 가는 행동을 선택하여 수행했다면 다음 타임스텝에 무조건 왼쪽으로 간다. 즉, 행동이 반드시 수행된다.

$v_{\pi}(s) = \sum\limits_{a\in A}\pi(a \| s)(r_{(s, a)} + \gamma v_{\pi}(s^{\prime}))$

상태 변환 확률이 1일 경우의 벨만 기대 방정식은 다음과 같이 계산된다:

1. 각 행동에 대해 그 행동을 할 확률을 고려
2. 각 행동을 했을 때 받을 보상을 고려
3. 다음 상태의 가치함수를 고려

벨만 기대 방정식을 이용해 현재의 가치함수를 업데이트하다 보면 참 값을 구할 수 있다. 참 값은 최대로 받을 보상을 이야기하는 것이 아니라 현재의 정책을 따라갔을 경우에 에이전트가 얻을 실제 보상의 값에 대한 참 기댓값이다.

---

## 벨만 최적 방정식

### 참 가치함수의 수렴

벨만 기대방정식을 통해 계속 계산을 진행하다보면 식의 왼쪽 항과 오른쪽 항이 동일해진다.

- 처음 가치함수의 값들은 의미가 없는 값으로 초기화된다
- 초깃값으로부터 시작해 벨만 기대 방정식으로 반복적으로 계산하면 왼쪽 식과 오른쪽 식이 같아진다
- 즉, $v_{\pi}(s)$ 값이 수렴한다
- 이것은 현재 정책 $\pi$에 대한 참 가치함수를 구한 것이다

### 참 가치함수와 최적 가치함수

참 가치함수와 최적 가치함수는 다르다.

- **참 가치함수**: 어떤 정책을 따라 움직였을 경우 받게 되는 보상에 대한 참 값
- **최적 가치함수**: 수많은 정책 중에서 가장 높은 보상을 얻게 되는 정책을 따랐을 때의 가치함수

### 가치함수 업데이트 과정

$v_{k+1}(s) \leftarrow \sum\limits_{a\in A}\pi(a \| s)(r_{(s, a)} + \gamma v_k(s^{\prime}))$

- $v_{k+1}(s)$는 현재 정책에 따라 k+1번째 계산한 가치함수를 뜻하고 그중 상태 s의 가치함수를 의미한다
- k+1번째의 가치함수는 k번째 가치함수 중에서 주변상태들 $s^{\prime}$을 이용해 구한다
- 위의 계산은 모든 상태에 대해 동시에 진행한다
- 상태집합에 속한 모든 상태에 대해 가능한 행동들을 고려하며 주변 상태에 저장돼 있는 가치함수를 통해 현재의 가치함수를 업데이트한다
- 이를 통해 현재 정책에 대한 참 가치함수를 구할 수 있다

### 최적 가치함수와 최적 정책

$v_*(s) = \underset{\pi}{max}[v_{\pi}(s)]$

정책을 따라갔을 때 받을 보상들의 합인 가치함수를 통해 더 좋은 정책을 판단할 수 있다. 모든 정책에 대해 가장 큰 가치함수를 주는 정책이 최적 정책이다.

$q_*(s,a) = \underset{\pi}{max}[q_{\pi}(s,a)]$

가장 높은 가치함수를 에이전트가 찾았다고 가정했을 때 최적 정책은 각 상태 s에서의 최적의 큐함수 중에서 가장 큰 큐함수를 가진 행동을 하는 것이다. 선택 상황에서 판단 기준은 큐함수이며 최적 정책은 언제나 이 큐함수 중에서 가장 높은 행동을 하는 것이다.


$\pi_*(s,a) = {1 \;\; \text{if} \;\; a=argmax_{a \in A} a_ * \brace 0 \quad \text{otherwise}}$

argmax는 $q_*$를 최대로 해주는 행동 a를 반환하는 함수다.

### 벨만 최적 방정식의 도출

벨만 방정식은 현재 상태의 가치함수와 다음 타임스텝 상태의 가치함수 사이의 관계를 나타낸다.

현재 상태의 가치함수가 최적이라고 가정해보자:

- 에이전트가 가장 좋은 행동을 선택한다는 뜻이다
- 에이전트는 큐함수를 기준으로 행동을 선택한다
- 이때 선택의 기준이 되는 큐함수가 최적의 큐함수가 아니라면 에이전트가 큐함수 중 최대를 선택하여도 가치함수는 최적의 가치함수가 되지 않는다
- 따라서 최적의 큐함수 중에서 max를 취하는 것이 최적의 가치함수가 된다

$v_*(s) = \underset{a}{max}[q_ *(s,a)]$

큐함수 중에서 최대를 선택하는 최적 가치함수다.

$v_*(s) = \underset{a}{max}E[R_{t+1} + \gamma v_ *(S_{t+1}) \| S_t = s, A_t = a]$

이것이 바로 **벨만 최적 방정식**이다. 최적의 가치함수에 대한 식이다.

### 큐함수의 벨만 최적 방정식

$q_*(s,a) = E[R_{t+1}  + \gamma \underset{a^{\prime}}{max}q_ *(S_{t+1}, a^{\prime}) \| S_t = s, A_t =a]$

큐함수에 대한 벨만 최적 방정식이다. 최적 정책을 따라갈 때 현재 상태의 큐함수는 다음 상태에서 선택 가능한 행동 중에서 가장 높은 값의 큐함수를 1번 할인하고 보상을 더한 것과 같다. E가 있는 이유는 다음 상태가 상태 변환 확률에 따라 달라지기 때문이다.

---

## 다이내믹 프로그래밍과의 연결

벨만 기대 방정식과 벨만 최적 방정식을 이용해 MDP로 정의되는 문제를 "계산"으로 푸는 방법이 바로 **다이내믹 프로그래밍**이다.

---

## 정리

### MDP
- 순차적 행동 결정 문제를 수학적으로 정의한 것이다
- 상태, 행동, 보상함수, 상태 변환 확률, 할인율로 구성된다
- 더 좋은 정책을 찾는 과정으로 해결한다

### 가치함수
- 에이전트가 어떤 정책이 더 좋은 정책인지 판단하는 기준이다
- 현재 상태로부터 정책을 따라갔을 때 받을 것이라 예상되는 보상의 합이다
- 에이전트는 정책을 업데이트할 때 가치함수를 사용하는데 보통은 에이전트가 선택할 각 행동의 가치를 직접적으로 나타내는 큐함수를 사용한다

### 벨만 방정식
- 현재 상태의 가치함수와 다음 상태의 가치함수의 관계식이다
- **벨만 기대 방정식**은 특정 정책을 따라갔을 때 가치함수 사이의 관계식이다
- 최적의 정책은 최적의 가치함수를 받게 하는 정책이며, 그때 가치함수 사이의 관계식이 **벨만 최적 방정식**이다
