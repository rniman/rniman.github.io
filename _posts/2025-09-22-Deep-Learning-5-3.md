---
title: ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 1 | ì˜¤ì°¨ì—­ì „íŒŒë²• 3
date: 2025-09-22 23:14:25 +0900
categories: [Study, Study\DeepLearning]
tags: [AI, Deep Learning, Python, Book Note]
author: "rniman"                            # for single entry
# or
# authors: [<author1_id>, <author2_id>]   # for multiple entries
description: "ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 1 Chapter 5 ì •ë¦¬"
math: true
comments: false # ëŒ“ê¸€ ê¸°ëŠ¥
---

## ì±… ì •ë³´ ğŸ“– 

- ì±… ì œëª©: ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 1
- ê¸€ì“´ì´: ì‚¬ì´í†  ê³ í‚¤
- ì˜®ê¸´ì´: ê°œì•ë§µì‹œ
- ì¶œíŒì‚¬: í•œë¹›ë¯¸ë””ì–´
- ë°œí–‰ì¼: 2025ë…„ 01ì›” 24ì¼ 
- ì±•í„°: Chapter 5. ì˜¤ì°¨ì—­ì „íŒŒë²•

### ì±…ì†Œê°œ
ë”¥ëŸ¬ë‹ ë¶„ì•¼ ë¶€ë™ì˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬!
ë¨¸ë¦¬ë¡œ ì´í•´í•˜ê³  ì†ìœ¼ë¡œ ìµíˆëŠ” ê°€ì¥ ì‰¬ìš´ ë”¥ëŸ¬ë‹ ì…ë¬¸ì„œ
ì´ ì±…ì€ ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ ê°œë…ì„ â€˜ë°‘ë°”ë‹¥ë¶€í„°â€™ êµ¬í˜„í•´ë³´ë©° ê¸°ì´ˆë¥¼ í•œ ê±¸ìŒì”© íƒ„íƒ„í•˜ê²Œ ë‹¤ì§ˆ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ì¹œì ˆí•œ ì•ˆë‚´ì„œì…ë‹ˆë‹¤. ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ í”„ë ˆì„ì›Œí¬ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ë”¥ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ë¶€í„° ì´ë¯¸ì§€ ì¸ì‹ì— í™œìš©ë˜ëŠ” í•©ì„±ê³± ì‹ ê²½ë§(CNN)ê¹Œì§€ ë”¥ëŸ¬ë‹ì˜ ì›ë¦¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤. ë˜í•œ ë³µì¡í•œ ê°œë…ì€ ê³„ì‚° ê·¸ë˜í”„ë¥¼ í™œìš©í•´ ì‹œê°ì ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì±…ì€ ë”¥ëŸ¬ë‹ì— ì²«ë°œì„ ë‚´ë”›ëŠ” ì…ë¬¸ìëŠ” ë¬¼ë¡ ì´ê³  ê¸°ì´ˆë¥¼ ë‹¤ì‹œê¸ˆ ë‹¤ì§€ê³  ì‹¶ì€ ê°œë°œìì™€ ì—°êµ¬ìì—ê²Œë„ í›Œë¥­í•œ ê¸¸ì¡ì´ê°€ ë˜ì–´ì¤„ ê²ƒì…ë‹ˆë‹¤.

* ì¶œì²˜ : [ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 1 - êµë³´ë¬¸ê³ ](https://product.kyobobook.co.kr/detail/S000215599933)

### ì£¼ìš” ë‚´ìš©
- ì™„ì „í•œ ì˜¤ì°¨ì—­ì „íŒŒë²• êµ¬í˜„: ì´ë¡ ì—ì„œ ì‹¤ì „ê¹Œì§€

ê³„ì¸µë³„ êµ¬í˜„ì„ ë§ˆì³¤ë‹¤ë©´ ì´ì œ ì´ë“¤ì„ ì¡°í•©í•˜ì—¬ ì™„ì „í•œ ì‹ ê²½ë§ì„ êµ¬í˜„í•  ì°¨ë¡€ë‹¤. ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì‚¬ìš©í•˜ë©´ ìˆ˜ì¹˜ ë¯¸ë¶„ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ì‹¤ì œ ë™ì‘í•˜ëŠ” ì‹ ê²½ë§ì„ êµ¬í˜„í•˜ê³  ê²€ì¦í•´ë³´ì.

---

## ì‹ ê²½ë§ í•™ìŠµì˜ ì „ì²´ ê·¸ë¦¼

ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì ìš©í•˜ê¸° ì „ì— ì‹ ê²½ë§ í•™ìŠµì˜ ì „ì²´ ê³¼ì •ì„ ë‹¤ì‹œ ì •ë¦¬í•´ë³´ì:

### í•™ìŠµ ê³¼ì •ì˜ 5ë‹¨ê³„

#### 1. ì „ì œ
ì‹ ê²½ë§ì—ëŠ” ì ì‘ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì´ ìˆê³ , ì´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í›ˆë ¨ ë°ì´í„°ì— ì ì‘í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ê³¼ì •ì„ 'í•™ìŠµ'ì´ë¼ í•œë‹¤.

#### 2. ë¯¸ë‹ˆë°°ì¹˜
í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì˜¨ë‹¤. ì´ë ‡ê²Œ ì„ ë³„í•œ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¼ í•˜ë©°, ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œë‹¤.

#### 3. ê¸°ìš¸ê¸° ì‚°ì¶œ
ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì´ê¸° ìœ„í•´ ê° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤. ê¸°ìš¸ê¸°ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°€ì¥ ì‘ê²Œ í•˜ëŠ” ë°©í–¥ì„ ì œì‹œí•œë‹¤.

#### 4. ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì•„ì£¼ ì¡°ê¸ˆ ê°±ì‹ í•œë‹¤.

#### 5. ë°˜ë³µ
1~3ë‹¨ê³„ë¥¼ ë°˜ë³µí•œë‹¤.

**ì¤‘ìš”í•œ ì **: ì˜¤ì°¨ì—­ì „íŒŒë²•ì´ ë“±ì¥í•˜ëŠ” ë‹¨ê³„ëŠ” 'ê¸°ìš¸ê¸° ì‚°ì¶œ'ì´ë‹¤. ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì´ìš©í•˜ë©´ ëŠë¦° ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ ë‹¬ë¦¬ ê¸°ìš¸ê¸°ë¥¼ íš¨ìœ¨ì ì´ê³  ë¹ ë¥´ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.

---

## ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì ìš©í•œ ì‹ ê²½ë§ êµ¬í˜„

ì´ì „ ì¥ì—ì„œ êµ¬í˜„í•œ ê° ê³„ì¸µë“¤ì„ ì¡°í•©í•˜ì—¬ ì™„ì „í•œ 2ì¸µ ì‹ ê²½ë§ì„ êµ¬í˜„í•œë‹¤.

```python
import numpy as np
from collections import OrderedDict

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) 
        self.params['b2'] = np.zeros(output_size)

        # ê³„ì¸µ ìƒì„± (ìˆœì„œê°€ ì¤‘ìš”í•˜ë¯€ë¡œ OrderedDict ì‚¬ìš©)
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])

        self.lastLayer = SoftmaxWithLoss()
        
    def predict(self, x):
        # ìˆœì „íŒŒ: ê° ê³„ì¸µì„ ìˆœì„œëŒ€ë¡œ í†µê³¼
        for layer in self.layers.values():
            x = layer.forward(x)
        return x
        
    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1: 
            t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
        
    def numerical_gradient(self, x, t):
        # ìˆ˜ì¹˜ ë¯¸ë¶„ ë°©ì‹ (ë¹„êµìš©)
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
        
    def gradient(self, x, t):
        # ì˜¤ì°¨ì—­ì „íŒŒë²• ë°©ì‹ (íš¨ìœ¨ì )
        
        # ìˆœì „íŒŒ
        self.loss(x, t)

        # ì—­ì „íŒŒ
        dout = 1
        dout = self.lastLayer.backward(dout)
        
        # ê³„ì¸µì„ ì—­ìˆœìœ¼ë¡œ í†µê³¼
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        # ê¸°ìš¸ê¸° ì €ì¥
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db

        return grads
```

### êµ¬í˜„ì˜ í•µì‹¬ ìš”ì†Œ

**1. params - ë”•ì…”ë„ˆë¦¬**
- ì‹ ê²½ë§ì˜ ë§¤ê°œë³€ìˆ˜(ê°€ì¤‘ì¹˜, í¸í–¥)ë¥¼ ë³´ê´€í•œë‹¤

**2. layers - OrderedDict**
- ì‹ ê²½ë§ì˜ ê³„ì¸µì„ ë³´ê´€í•˜ëŠ” **ìˆœì„œê°€ ìˆëŠ”** ë”•ì…”ë„ˆë¦¬
- ìˆœì „íŒŒ ì‹œ ì¶”ê°€í•œ ìˆœì„œëŒ€ë¡œ `forward()` í˜¸ì¶œ
- ì—­ì „íŒŒ ì‹œ `layers.reverse()`ë¡œ ë°˜ëŒ€ ìˆœì„œë¡œ `backward()` í˜¸ì¶œ

**3. lastLayer**
- ì‹ ê²½ë§ì˜ ë§ˆì§€ë§‰ ê³„ì¸µ (SoftmaxWithLoss)
- ì†ì‹¤ ê³„ì‚°ê³¼ ì—­ì „íŒŒ ì‹œì‘ì  ì—­í• 

---

## ê¸°ìš¸ê¸° ê²€ì¦: êµ¬í˜„ì˜ ì •í™•ì„± í™•ì¸

ì˜¤ì°¨ì—­ì „íŒŒë²•ì´ ì œëŒ€ë¡œ êµ¬í˜„ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ë ¤ë©´ **ê¸°ìš¸ê¸° í™•ì¸(Gradient Check)**ì„ ìˆ˜í–‰í•´ì•¼ í•œë‹¤.

### ê¸°ìš¸ê¸° í™•ì¸ì˜ í•„ìš”ì„±

- **ìˆ˜ì¹˜ ë¯¸ë¶„**: ëŠë¦¬ì§€ë§Œ êµ¬í˜„ì´ ì‰½ê³  ê±°ì˜ í™•ì‹¤í•˜ê²Œ ì˜¬ë°”ë¥¸ ê²°ê³¼
- **ì˜¤ì°¨ì—­ì „íŒŒë²•**: ë¹ ë¥´ì§€ë§Œ êµ¬í˜„ì´ ë³µì¡í•˜ì—¬ ë²„ê·¸ ê°€ëŠ¥ì„± ì¡´ì¬
- **í•´ê²°ì±…**: ë‘ ë°©ë²•ìœ¼ë¡œ êµ¬í•œ ê¸°ìš¸ê¸°ë¥¼ ë¹„êµí•˜ì—¬ ì¼ì¹˜í•¨ì„ í™•ì¸

```python
# ë°ì´í„° ì½ê¸°
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# ì†ŒëŸ‰ì˜ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
x_batch = x_train[:3]
t_batch = t_train[:3]

# ë‘ ë°©ì‹ìœ¼ë¡œ ê¸°ìš¸ê¸° ê³„ì‚°
grad_numerical = network.numerical_gradient(x_batch, t_batch)
grad_backprop = network.gradient(x_batch, t_batch)

# ê° ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ€ ì˜¤ì°¨ì˜ í‰ê· ì„ êµ¬í•œë‹¤
for key in grad_numerical.keys():
    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))
    print(f"{key}: {diff}")
```

### ê²€ì¦ ê²°ê³¼ í•´ì„

```
W1: 4.3057892579125414e-10
b1: 2.9001505542469284e-09
W2: 5.623646449127339e-09
b2: 1.4047384122028995e-07
```

**ê²°ê³¼ ë¶„ì„:**
- ëª¨ë“  ì˜¤ì°¨ê°€ ë§¤ìš° ì‘ë‹¤ (10^-7 ~ 10^-10 ìˆ˜ì¤€)
- ì´ëŠ” ì˜¤ì°¨ì—­ì „íŒŒë²•ì´ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë˜ì—ˆìŒì„ ì˜ë¯¸í•œë‹¤
- ì™„ì „íˆ 0ì´ ì•„ë‹Œ ì´ìœ ëŠ” ìˆ˜ì¹˜ ë¯¸ë¶„ì˜ ê·¼ì‚¬ ì˜¤ì°¨ ë•Œë¬¸ì´ë‹¤

---

## ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì‚¬ìš©í•œ ì‹¤ì œ í•™ìŠµ

ì´ì œ ê²€ì¦ëœ ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ í•™ìŠµì„ ìˆ˜í–‰í•´ë³´ì.

```python
# ë°ì´í„° ì½ê¸°
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    # ë¯¸ë‹ˆë°°ì¹˜ ìƒì„±
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # ê¸°ìš¸ê¸° ê³„ì‚° (ì˜¤ì°¨ì—­ì „íŒŒë²• ì‚¬ìš©)
    grad = network.gradient(x_batch, t_batch)
    
    # ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    # í•™ìŠµ ê²½ê³¼ ê¸°ë¡
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # 1ì—í¬í¬ë§ˆë‹¤ ì •í™•ë„ í‰ê°€
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print(f"Epoch {i//iter_per_epoch}: Train Acc {train_acc:.4f}, Test Acc {test_acc:.4f}")
```

### ì„±ëŠ¥ ë¹„êµ

**ìˆ˜ì¹˜ ë¯¸ë¶„ vs ì˜¤ì°¨ì—­ì „íŒŒë²•:**
- ìˆ˜ì¹˜ ë¯¸ë¶„: ë§¤ê°œë³€ìˆ˜ í•˜ë‚˜ë‹¹ ì—¬ëŸ¬ ë²ˆì˜ ìˆœì „íŒŒ í•„ìš” â†’ ë§¤ìš° ëŠë¦¼
- ì˜¤ì°¨ì—­ì „íŒŒë²•: ìˆœì „íŒŒ 1ë²ˆ + ì—­ì „íŒŒ 1ë²ˆ â†’ ë§¤ìš° ë¹ ë¦„

---

## í•µì‹¬ ê°œë… ì •ë¦¬

### ê³„ì‚° ê·¸ë˜í”„ì˜ í˜

**ì‹œê°ì  ì´í•´:**
- ê³„ì‚° ê³¼ì •ì„ ë…¸ë“œì™€ ì—ì§€ë¡œ ì‹œê°í™”
- ë³µì¡í•œ ê³„ì‚°ì„ ë‹¨ìˆœí•œ êµ­ì†Œ ê³„ì‚°ìœ¼ë¡œ ë¶„í•´

**íš¨ìœ¨ì  ë¯¸ë¶„:**
- ì—­ì „íŒŒë¥¼ í†µí•´ ëª¨ë“  ë³€ìˆ˜ì˜ ë¯¸ë¶„ì„ í•œ ë²ˆì— ê³„ì‚°
- ì—°ì‡„ë²•ì¹™ì„ ìë™ìœ¼ë¡œ ì ìš©

### ê³„ì¸µí™”ì˜ ì¥ì 

**ëª¨ë“ˆì„±:**
- ê° ê³„ì¸µì´ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘
- `forward()`ì™€ `backward()` ì¸í„°í˜ì´ìŠ¤ í†µì¼

**í™•ì¥ì„±:**
- ìƒˆë¡œìš´ ê³„ì¸µì„ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥
- ê¹Šì€ ì‹ ê²½ë§ìœ¼ë¡œ í™•ì¥ ìš©ì´

**ì¬ì‚¬ìš©ì„±:**
- í•œ ë²ˆ êµ¬í˜„í•œ ê³„ì¸µì„ ì—¬ëŸ¬ ê³³ì—ì„œ ì‚¬ìš© ê°€ëŠ¥

### êµ¬í˜„ ê²€ì¦ì˜ ì¤‘ìš”ì„±

**ê¸°ìš¸ê¸° í™•ì¸:**
- ì˜¤ì°¨ì—­ì „íŒŒë²• êµ¬í˜„ì˜ ì •í™•ì„± ê²€ì¦
- ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ì˜ ë¹„êµë¥¼ í†µí•œ ë””ë²„ê¹…

**ì ì§„ì  ê°œë°œ:**
- ê° ê³„ì¸µì„ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
- ì „ì²´ ë„¤íŠ¸ì›Œí¬ ì¡°ë¦½ í›„ ìµœì¢… ê²€ì¦

---

## ë§ˆë¬´ë¦¬

ì˜¤ì°¨ì—­ì „íŒŒë²•ì€ ì‹ ê²½ë§ í•™ìŠµì˜ í•µì‹¬ ê¸°ìˆ ì´ë‹¤. ê³„ì‚° ê·¸ë˜í”„ì™€ ì—°ì‡„ë²•ì¹™ì„ ë°”íƒ•ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ê¸°ìš¸ê¸° ê³„ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

**í•µì‹¬ í¬ì¸íŠ¸:**
- **ê³„ì‚° ê·¸ë˜í”„**: ë³µì¡í•œ ê³„ì‚°ì˜ ì‹œê°ì  í‘œí˜„ê³¼ ìë™ ë¯¸ë¶„
- **ê³„ì¸µí™”**: ëª¨ë“ˆí™”ëœ ì‹ ê²½ë§ êµ¬ì¡°ì™€ ì¸í„°í˜ì´ìŠ¤ í†µì¼  
- **íš¨ìœ¨ì„±**: ìˆ˜ì¹˜ ë¯¸ë¶„ ëŒ€ë¹„ ì••ë„ì ì¸ ì†ë„ í–¥ìƒ
- **ê²€ì¦**: ê¸°ìš¸ê¸° í™•ì¸ì„ í†µí•œ êµ¬í˜„ ì •í™•ì„± ë³´ì¥

