---
title: ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 1 | ì˜¤ì°¨ì—­ì „íŒŒë²• 2
date: 2025-09-22 20:44:44 +0900
categories: [Study, Study\DeepLearning]
tags: [AI, Deep Learning, Python, Book Note]
author: "rniman"                            # for single entry
# or
# authors: [<author1_id>, <author2_id>]   # for multiple entries
description: "ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 1 Chapter 5 ì •ë¦¬"
math: true
comments: false # ëŒ“ê¸€ ê¸°ëŠ¥
---

## ì±… ì •ë³´ ğŸ“– 

- ì±… ì œëª©: ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 1
- ê¸€ì“´ì´: ì‚¬ì´í†  ê³ í‚¤
- ì˜®ê¸´ì´: ê°œì•ë§µì‹œ
- ì¶œíŒì‚¬: í•œë¹›ë¯¸ë””ì–´
- ë°œí–‰ì¼: 2025ë…„ 01ì›” 24ì¼ 
- ì±•í„°: Chapter 5. ì˜¤ì°¨ì—­ì „íŒŒë²•

### ì±…ì†Œê°œ
ë”¥ëŸ¬ë‹ ë¶„ì•¼ ë¶€ë™ì˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬!
ë¨¸ë¦¬ë¡œ ì´í•´í•˜ê³  ì†ìœ¼ë¡œ ìµíˆëŠ” ê°€ì¥ ì‰¬ìš´ ë”¥ëŸ¬ë‹ ì…ë¬¸ì„œ
ì´ ì±…ì€ ë”¥ëŸ¬ë‹ì˜ í•µì‹¬ ê°œë…ì„ â€˜ë°‘ë°”ë‹¥ë¶€í„°â€™ êµ¬í˜„í•´ë³´ë©° ê¸°ì´ˆë¥¼ í•œ ê±¸ìŒì”© íƒ„íƒ„í•˜ê²Œ ë‹¤ì§ˆ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ì¹œì ˆí•œ ì•ˆë‚´ì„œì…ë‹ˆë‹¤. ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ í”„ë ˆì„ì›Œí¬ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ë”¥ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ë¶€í„° ì´ë¯¸ì§€ ì¸ì‹ì— í™œìš©ë˜ëŠ” í•©ì„±ê³± ì‹ ê²½ë§(CNN)ê¹Œì§€ ë”¥ëŸ¬ë‹ì˜ ì›ë¦¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤. ë˜í•œ ë³µì¡í•œ ê°œë…ì€ ê³„ì‚° ê·¸ë˜í”„ë¥¼ í™œìš©í•´ ì‹œê°ì ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì±…ì€ ë”¥ëŸ¬ë‹ì— ì²«ë°œì„ ë‚´ë”›ëŠ” ì…ë¬¸ìëŠ” ë¬¼ë¡ ì´ê³  ê¸°ì´ˆë¥¼ ë‹¤ì‹œê¸ˆ ë‹¤ì§€ê³  ì‹¶ì€ ê°œë°œìì™€ ì—°êµ¬ìì—ê²Œë„ í›Œë¥­í•œ ê¸¸ì¡ì´ê°€ ë˜ì–´ì¤„ ê²ƒì…ë‹ˆë‹¤.

* ì¶œì²˜ : [ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 1 - êµë³´ë¬¸ê³ ](https://product.kyobobook.co.kr/detail/S000215599933)

### ì£¼ìš” ë‚´ìš©
- ì˜¤ì°¨ì—­ì „íŒŒë²•: íš¨ìœ¨ì ì¸ ê¸°ìš¸ê¸° ê³„ì‚°ì˜ í•µì‹¬

ê³„ì‚° ê·¸ë˜í”„ì™€ ì—°ì‡„ë²•ì¹™ì˜ ì›ë¦¬ë¥¼ ì´í•´í–ˆë‹¤ë©´ ì´ì œ ì‹¤ì œ ì‹ ê²½ë§ì˜ ê° êµ¬ì„± ìš”ì†Œë¥¼ ê³„ì¸µ(Layer)ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì. ê° ê³„ì¸µì€ ìˆœì „íŒŒì™€ ì—­ì „íŒŒë¥¼ ëª¨ë‘ êµ¬í˜„í•˜ì—¬ íš¨ìœ¨ì ì¸ ê¸°ìš¸ê¸° ê³„ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

---

## ë‹¨ìˆœí•œ ê³„ì¸µ êµ¬í˜„í•˜ê¸°

### ê³±ì…ˆ ê³„ì¸µ(Multiplication Layer)

ê³±ì…ˆ ë…¸ë“œì˜ ì—­ì „íŒŒëŠ” ìˆœì „íŒŒì˜ ì…ë ¥ ì‹ í˜¸ë¥¼ ì„œë¡œ ë°”ê¾¼ ê°’ì„ ê³±í•´ì„œ í•˜ë¥˜ë¡œ ë³´ë‚¸ë‹¤.

```python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None

    def forward(self, x, y):
        self.x = x  # ì—­ì „íŒŒë¥¼ ìœ„í•´ ìˆœì „íŒŒ ì…ë ¥ ì €ì¥
        self.y = y                
        out = x * y
        return out

    def backward(self, dout):
        dx = dout * self.y  # xì™€ yë¥¼ ë°”ê¾¼ë‹¤
        dy = dout * self.x
        return dx, dy
```

**í•µì‹¬ í¬ì¸íŠ¸:**
- `forward()`: xì™€ yë¥¼ ê³±í•œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë©°, ì…ë ¥ê°’ì„ ì €ì¥í•œë‹¤
- `backward()`: ìƒë¥˜ì—ì„œ ì˜¨ ë¯¸ë¶„(`dout`)ì— ìˆœì „íŒŒ ë•Œì˜ ê°’ì„ ì„œë¡œ ë°”ê¿” ê³±í•œë‹¤
- ê³±ì…ˆ ë…¸ë“œëŠ” **ìˆœì „íŒŒì˜ ì…ë ¥ê°’ì´ í•„ìš”**í•˜ë¯€ë¡œ ë°˜ë“œì‹œ ì €ì¥í•´ì•¼ í•œë‹¤

### ë§ì…ˆ ê³„ì¸µ(Addition Layer)

ë§ì…ˆ ë…¸ë“œì˜ ì—­ì „íŒŒëŠ” ìƒë¥˜ì—ì„œ ì˜¨ ë¯¸ë¶„ì„ ê·¸ëŒ€ë¡œ í•˜ë¥˜ë¡œ ì „ë‹¬í•œë‹¤.

```python
class AddLayer:
    def __init__(self):
        pass  # ì €ì¥í•  ê°’ì´ ì—†ìŒ

    def forward(self, x, y):
        return x + y

    def backward(self, dout):
        dx = dout * 1  # ê·¸ëŒ€ë¡œ ì „ë‹¬
        dy = dout * 1  # ê·¸ëŒ€ë¡œ ì „ë‹¬
        return dx, dy
```

---

## í™œì„±í™” í•¨ìˆ˜ ê³„ì¸µ êµ¬í˜„í•˜ê¸°

### ReLU ê³„ì¸µ

ReLU í•¨ìˆ˜ì™€ ê·¸ ë¯¸ë¶„:

$$y = \begin{cases}x & (x > 0) \\0 & (x \leq 0)\end{cases}$$

$$\frac{\partial y}{\partial x} = \begin{cases}1 & (x > 0) \\0 & (x \leq 0)\end{cases}$$

```python
class Relu:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        self.mask = (x <= 0)  # 0 ì´í•˜ì¸ ìœ„ì¹˜ë¥¼ Trueë¡œ í‘œì‹œ
        out = x.copy()
        out[self.mask] = 0    # 0 ì´í•˜ì¸ ìœ„ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
        return out

    def backward(self, dout):
        dout[self.mask] = 0   # ìˆœì „íŒŒ ë•Œ 0 ì´í•˜ì˜€ë˜ ìœ„ì¹˜ëŠ” 0ìœ¼ë¡œ
        dx = dout
        return dx
```

**ReLU ì—­ì „íŒŒì˜ íŠ¹ì§•:**
- ìˆœì „íŒŒì—ì„œ x > 0ì´ë©´ ì—­ì „íŒŒì—ì„œ ê·¸ëŒ€ë¡œ í†µê³¼
- ìˆœì „íŒŒì—ì„œ x â‰¤ 0ì´ë©´ ì—­ì „íŒŒì—ì„œ ì‹ í˜¸ ì°¨ë‹¨ (0 ì „ë‹¬)
- `mask` ë°°ì—´ì„ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ë²¡í„° ì—°ì‚° ìˆ˜í–‰

### Sigmoid ê³„ì¸µ

Sigmoid í•¨ìˆ˜: $y = \frac{1}{1+\exp(-x)}$

![Sigmoid ê³„ì¸µì˜ ê³„ì‚° ê·¸ë˜í”„(ìˆœì „íŒŒ)](assets/img/Book/Deep-Learning/Sigmoid Forward Propagation.png)
_Sigmoid ê³„ì¸µì˜ ê³„ì‚° ê·¸ë˜í”„(ìˆœì „íŒŒ)_

#### ì—­ì „íŒŒ ê³¼ì • ë¶„ì„

![Sigmoid ê³„ì¸µì˜ ê³„ì‚° ê·¸ë˜í”„](assets/img/Book/Deep-Learning/Sigmoid Forward & Backward Propagation.png)
_Sigmoid ê³„ì¸µì˜ ê³„ì‚° ê·¸ë˜í”„_

ë³µì¡í•œ ê³„ì‚° ê·¸ë˜í”„ë¥¼ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•˜ë©´:

1. **'/' ë…¸ë“œ**: $y = \frac{1}{x}$ì˜ ë¯¸ë¶„ì€ $-\frac{1}{x^2} = -y^2$
2. **'+' ë…¸ë“œ**: ìƒë¥˜ ê°’ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
3. **'exp' ë…¸ë“œ**: $y = \exp(x)$ì˜ ë¯¸ë¶„ì€ $\exp(x)$
4. **'Ã—' ë…¸ë“œ**: ìˆœì „íŒŒ ì…ë ¥ì„ ì„œë¡œ ë°”ê¿”ì„œ ê³±í•¨

ìµœì¢… ê²°ê³¼: $\frac{\partial L}{\partial y} y^2 \exp(-x) = \frac{\partial L}{\partial y} y(1-y)$

![Sigmoid ê³„ì¸µì˜ ê³„ì‚° ê·¸ë˜í”„(ê°„ì†Œí™” ë²„ì „)](assets/img/Book/Deep-Learning/Simply Sigmoid Propagation.png)
_Sigmoid ê³„ì¸µì˜ ê³„ì‚° ê·¸ë˜í”„(ê°„ì†Œí™” ë²„ì „)_

```python
class Sigmoid:
    def __init__(self):
        self.out = None

    def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out  # ì—­ì „íŒŒë¥¼ ìœ„í•´ ì¶œë ¥ ì €ì¥
        return out

    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out  # y(1-y)
        return dx
```

**Sigmoid ì—­ì „íŒŒì˜ íŠ¹ì§•:**
- ë³µì¡í•œ ì¤‘ê°„ ê³„ì‚° ê³¼ì •ì„ ê°„ì†Œí™”í•  ìˆ˜ ìˆë‹¤
- ìˆœì „íŒŒì˜ ì¶œë ¥ê°’(`self.out`)ë§Œìœ¼ë¡œ ì—­ì „íŒŒ ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤
- `y(1-y)` í˜•íƒœë¡œ ë§¤ìš° ê°„ë‹¨í•˜ê²Œ í‘œí˜„ëœë‹¤

---

## Affine/Softmax ê³„ì¸µ êµ¬í˜„í•˜ê¸°

### Affine ê³„ì¸µ

ì‹ ê²½ë§ì˜ ìˆœì „íŒŒì—ì„œ ìˆ˜í–‰í•˜ëŠ” í–‰ë ¬ ê³±ì€ ê¸°í•˜í•™ì—ì„œ **Affine Transformation**ì´ë¼ê³  í•œë‹¤.

![Affine ê³„ì¸µì˜ ê³„ì‚° ê·¸ë˜í”„: ë³€ìˆ˜ê°€ í–‰ë ¬ì„ì— ì£¼ì˜.](assets/img/Book/Deep-Learning/Affine Layer Computatational Graph.png)
_Affine ê³„ì¸µì˜ ê³„ì‚° ê·¸ë˜í”„: ë³€ìˆ˜ê°€ í–‰ë ¬ì„ì— ì£¼ì˜._

**ìˆ˜ì‹:** $Y = X \cdot W + B$

**ì—­ì „íŒŒ ê³µì‹:**
- $\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \cdot W^T$
- $\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Y}$
- $\frac{\partial L}{\partial B} = \frac{\partial L}{\partial Y}$

### ë°°ì¹˜ìš© Affine ê³„ì¸µ

```python
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        self.original_x_shape = None
        # ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì˜ ë¯¸ë¶„
        self.dW = None
        self.db = None

    def forward(self, x):
        # ë‹¤ì°¨ì› ì…ë ¥ì„ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜
        self.original_x_shape = x.shape
        x = x.reshape(x.shape[0], -1)
        self.x = x

        out = np.dot(self.x, self.W) + self.b
        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)  # ë°°ì¹˜ ì°¨ì›ì—ì„œ í•©ê³„
        
        # ì›ë˜ ì…ë ¥ í˜•ìƒìœ¼ë¡œ ë³µì›
        dx = dx.reshape(*self.original_x_shape)
        return dx
```

**ë°°ì¹˜ ì²˜ë¦¬ì˜ í•µì‹¬:**
- **í¸í–¥ì˜ ì—­ì „íŒŒ**: ê° ë°ì´í„°ì˜ ì—­ì „íŒŒ ê°’ì´ í¸í–¥ì˜ ì›ì†Œì— ëª¨ì—¬ì•¼ í•œë‹¤
- `axis=0`ìœ¼ë¡œ ë°°ì¹˜ ì°¨ì›ì—ì„œ í•©ê³„ë¥¼ êµ¬í•œë‹¤
- **í˜•ìƒ ë³€í™˜**: ë‹¤ì°¨ì› ì…ë ¥ì„ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜ í›„ ì›ë˜ í˜•ìƒìœ¼ë¡œ ë³µì›

---

## Softmax-with-Loss ê³„ì¸µ

### ì¶”ë¡  vs í•™ìŠµì—ì„œì˜ Softmax

![image.png](assets/img/Book/Deep-Learning/MNIST Propagation Computatational Graph.png)

**ì¶”ë¡  ì‹œ:**
- Softmax ê³„ì¸µì´ ë¶ˆí•„ìš”í•˜ë‹¤
- ê°€ì¥ ë†’ì€ ì ìˆ˜(score)ë§Œ ì•Œë©´ ë˜ê¸° ë•Œë¬¸

**í•™ìŠµ ì‹œ:**
- Softmax ê³„ì¸µì´ í•„ìš”í•˜ë‹¤
- ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°ì„ ìœ„í•´ í™•ë¥  ë¶„í¬ê°€ í•„ìš”

### Softmax-with-Loss ê³„ì¸µì˜ ì—­ì „íŒŒ

![ê°„ì†Œí™”í•œ Softmax-with-Loss ê³„ì¸µì˜ ê³„ì‚° ê·¸ë˜í”„](assets/img/Book/Deep-Learning/Simply Softmax with Loss Computational Graph.png)
_ê°„ì†Œí™”í•œ Softmax-with-Loss ê³„ì¸µì˜ ê³„ì‚° ê·¸ë˜í”„_

**ë†€ë¼ìš´ ê²°ê³¼:**
Softmax ê³„ì¸µì˜ ì—­ì „íŒŒëŠ” $(y_1 - t_1, y_2 - t_2, y_3 - t_3)$ë¼ëŠ” **ë§ë”í•œ** ê²°ê³¼ë¥¼ ë‚¸ë‹¤.

**ì˜ë¯¸:**
- $(y_1, y_2, y_3)$: Softmax ê³„ì¸µì˜ ì¶œë ¥
- $(t_1, t_2, t_3)$: ì •ë‹µ ë ˆì´ë¸”
- **ì¶œë ¥ê³¼ ì •ë‹µì˜ ì°¨ì´**ê°€ ê·¸ëŒ€ë¡œ ì—­ì „íŒŒëœë‹¤

### ì™œ ì´ë ‡ê²Œ ê°„ë‹¨í• ê¹Œ?

ì´ëŠ” ìš°ì—°ì´ ì•„ë‹ˆë¼ **êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨**ê°€ ê·¸ë ‡ê²Œ ì„¤ê³„ë˜ì—ˆê¸° ë•Œë¬¸ì´ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ íšŒê·€ì—ì„œ í•­ë“± í•¨ìˆ˜ì™€ ì˜¤ì°¨ì œê³±í•©ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ë„ ê°™ë‹¤.

### êµ¬ì²´ì ì¸ ì˜ˆì‹œ

**ì˜ˆì‹œ 1: í° ì˜¤ì°¨**
- ì •ë‹µ: (0, 1, 0), ì¶œë ¥: (0.3, 0.2, 0.5)
- ì—­ì „íŒŒ: (0.3, -0.8, 0.5) â†’ í° ì˜¤ì°¨ ì „íŒŒ

**ì˜ˆì‹œ 2: ì‘ì€ ì˜¤ì°¨**  
- ì •ë‹µ: (0, 1, 0), ì¶œë ¥: (0.01, 0.99, 0.0)
- ì—­ì „íŒŒ: (0.01, -0.01, 0.0) â†’ ì‘ì€ ì˜¤ì°¨ ì „íŒŒ

### êµ¬í˜„

```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None  # ì†ì‹¤
        self.y = None     # softmax ì¶œë ¥
        self.t = None     # ì •ë‹µ ë ˆì´ë¸”
        
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        
        if self.t.size == self.y.size:  # ì›-í•« ì¸ì½”ë”©ì¸ ê²½ìš°
            dx = (self.y - self.t) / batch_size
        else:  # ë ˆì´ë¸” í˜•íƒœì¸ ê²½ìš°
            dx = self.y.copy()
            dx[np.arange(batch_size), self.t] -= 1
            dx = dx / batch_size
        
        return dx
```

---

## ë§ˆë¬´ë¦¬

ê° ê³„ì¸µì˜ êµ¬í˜„ì„ í†µí•´ ì˜¤ì°¨ì—­ì „íŒŒë²•ì˜ ì‹¤ì œ ë™ì‘ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤. ê° ê³„ì¸µì€ ë…ë¦½ì ìœ¼ë¡œ ìˆœì „íŒŒì™€ ì—­ì „íŒŒë¥¼ ìˆ˜í–‰í•˜ë©°, ì´ë“¤ì„ ì¡°í•©í•˜ì—¬ ì „ì²´ ì‹ ê²½ë§ì„ êµ¬ì„±í•œë‹¤.

**í•µì‹¬ í¬ì¸íŠ¸:**
- **ëª¨ë“ˆí™”**: ê° ê³„ì¸µì´ ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„ëœë‹¤
- **ì¸í„°í˜ì´ìŠ¤ í†µì¼**: ëª¨ë“  ê³„ì¸µì´ `forward()`ì™€ `backward()` ë©”ì„œë“œë¥¼ ê°€ì§„ë‹¤
- **íš¨ìœ¨ì„±**: ë³µì¡í•œ ê³„ì‚°ì„ ê°„ì†Œí™”ëœ í˜•íƒœë¡œ êµ¬í˜„í•œë‹¤
- **ìš°ì•„í•œ ì„¤ê³„**: Softmax-with-Lossì²˜ëŸ¼ ìˆ˜í•™ì ìœ¼ë¡œ ì•„ë¦„ë‹¤ìš´ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤

ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ì´ëŸ¬í•œ ê³„ì¸µë“¤ì„ ì¡°í•©í•˜ì—¬ ê°„ë‹¨í•œ ì‹ ê²½ë§ì„ êµ¬í˜„í•˜ê³  ì˜ˆì œë¥¼ ìˆ˜í–‰í•´ë³´ê² ë‹¤.
