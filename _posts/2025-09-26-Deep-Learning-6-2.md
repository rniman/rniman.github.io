---
title: 밑바닥부터 시작하는 딥러닝 1 | 학습 관련 기술들 2
date: 2025-09-26 17:05:25 +0900
categories: [Study, Study\DeepLearning]
tags: [AI, Deep Learning, Python, Book Note]
author: "rniman"                            # for single entry
# or
# authors: [<author1_id>, <author2_id>]   # for multiple entries
description: "밑바닥부터 시작하는 딥러닝 1 Chapter 6 정리"
math: true
comments: false # 댓글 기능
---

## 책 정보 📖 

- 책 제목: 밑바닥부터 시작하는 딥러닝 1
- 글쓴이: 사이토 고키
- 옮긴이: 개앞맵시
- 출판사: 한빛미디어
- 발행일: 2025년 01월 24일 
- 챕터: Chapter 6. 학습 관련 기술들 

### 책소개
딥러닝 분야 부동의 베스트셀러!
머리로 이해하고 손으로 익히는 가장 쉬운 딥러닝 입문서
이 책은 딥러닝의 핵심 개념을 ‘밑바닥부터’ 구현해보며 기초를 한 걸음씩 탄탄하게 다질 수 있도록 도와주는 친절한 안내서입니다. 라이브러리나 프레임워크에 의존하지 않고 딥러닝의 기본 개념부터 이미지 인식에 활용되는 합성곱 신경망(CNN)까지 딥러닝의 원리를 체계적으로 설명합니다. 또한 복잡한 개념은 계산 그래프를 활용해 시각적으로 전달하여 누구나 쉽게 이해할 수 있습니다. 이 책은 딥러닝에 첫발을 내딛는 입문자는 물론이고 기초를 다시금 다지고 싶은 개발자와 연구자에게도 훌륭한 길잡이가 되어줄 것입니다.

* 출처 : [밑바닥부터 시작하는 딥러닝 1 - 교보문고](https://product.kyobobook.co.kr/detail/S000215599933)

### 주요 내용
- 가중치 초기화와 배치 정규화: 학습 안정성의 핵심

신경망 학습에서 최적화 기법만큼 중요한 것이 **가중치 초기화**다. 가중치의 초깃값을 어떻게 설정하느냐가 학습의 성패를 가른다. 또한 **배치 정규화**는 이러한 초기화 의존성을 줄이고 학습을 더욱 안정적으로 만드는 혁신적인 기법이다.

---

## 가중치 초기화의 중요성

### 초깃값을 0으로 하면 안 되는 이유

가중치를 작게 만들고 싶다고 해서 초깃값을 0으로 설정하면 안 된다. 이는 매우 나쁜 아이디어다.

**문제점:**
- 오차역전파법에서 모든 가중치가 똑같이 갱신된다
- 가중치들이 같은 값으로 시작하면 갱신 후에도 같은 값을 유지한다
- 가중치를 여러 개 갖는 의미가 사라진다

**해결책:**
초깃값을 무작위로 설정해야 한다. 하지만 어떤 분포로 설정할지가 핵심이다.

---

## 활성화값 분포와 초기화 전략

### 은닉층의 활성화값 분포 관찰

각 층의 활성화값(활성화 함수의 출력) 분포를 관찰하면 초기화의 중요성을 확인할 수 있다.

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def ReLU(x):
    return np.maximum(0, x)

def tanh(x):
    return np.tanh(x)
    
# 실험 설정
input_data = np.random.randn(1000, 100)  # 1000개의 데이터
node_num = 100  # 각 은닉층의 뉴런 수
hidden_layer_size = 5  # 은닉층 5개
activations = {}  # 활성화 결과 저장

x = input_data

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1]

    # 다양한 초기화 방법 실험
    w = np.random.randn(node_num, node_num) * 1        # 표준편차 1
    # w = np.random.randn(node_num, node_num) * 0.01   # 표준편차 0.01
    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)  # Xavier
    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)  # He

    a = np.dot(x, w)
    z = sigmoid(a)  # 활성화 함수
    activations[i] = z
```

### 표준편차가 1인 정규분포 초기화

![가중치를 표준편차가 1인 정규분포로 초기화할 때의 각 층의 활성화값 분포](assets/img/Book/Deep-Learning/Weight Init std 1.png)
_가중치를 표준편차가 1인 정규분포로 초기화할 때의 각 층의 활성화값 분포_

**문제점:**
- 활성화값이 0과 1에 치우쳐 분포한다
- 시그모이드 함수의 출력이 0, 1에 가까우면 미분이 0에 근접한다
- **기울기 소실(Vanishing Gradient) 문제** 발생
- 깊은 신경망에서는 치명적인 문제가 된다

### 표준편차가 0.01인 정규분포 초기화

![가중치를 표준편차가 0.01인 정규분포로 초기화할 때의 각 층의 활성화값 분포](assets/img/Book/Deep-Learning/Weight Init std 0.01.png)
_가중치를 표준편차가 0.01인 정규분포로 초기화할 때의 각 층의 활성화값 분포_

**문제점:**
- 0.5 부근에 집중되어 기울기 소실 문제는 해결된다
- 하지만 활성화값이 치우친다는 새로운 문제 발생
- **표현력 제한**: 다수의 뉴런이 같은 값을 출력하므로 뉴런을 여러 개 둔 의미가 없어진다
- 학습이 비효율적으로 이루어진다

**핵심 원리:**
각 층의 활성화값은 **적당히 고루 분포**되어야 한다. 층과 층 사이에 적당하게 다양한 데이터가 흘러야 신경망 학습이 효율적으로 이루어진다.

---

## Xavier 초기화: 선형 활성화 함수를 위한 해결책

### Xavier 초기화의 원리

사비에르 글로로트와 요수아 벤지오가 제안한 가중치 초기화 방법이다.

**수식:** 앞 계층의 노드가 n개라면 표준편차가 $\frac{1}{\sqrt{n}}$인 분포를 사용한다.

**전제 조건:** 활성화 함수가 선형이라는 가정

### Xavier 초기화 결과

![가중치의 초깃값으로 'Xavier 초깃값'을 이용할 때의 각 층의 활성화값 분포](assets/img/Book/Deep-Learning/Weight Init Xavier.png)
_가중치의 초깃값으로 'Xavier 초깃값'을 이용할 때의 각 층의 활성화값 분포_

**개선된 점:**
- 층이 깊어지면서 형태가 다소 일그러지지만 넓게 분포된다
- 이전 방식들보다 훨씬 균등한 분포를 보인다
- sigmoid 대신 tanh 함수를 사용하면 더욱 개선된다

---

## He 초기화: ReLU를 위한 최적 해결책

### ReLU의 특성과 초기화

Xavier 초기화는 활성화 함수가 선형이라는 전제로 도출된 결과다. **ReLU**를 사용할 때는 다른 접근이 필요하다.

### He 초기화의 원리

카이밍 허(Kaiming He)가 제안한 ReLU 전용 초기화 방법이다.

**수식:** 앞 계층의 노드가 n개일 때 표준편차가 $\sqrt{\frac{2}{n}}$인 정규분포를 사용한다.

### He 초기화 결과

![활성화 함수로 ReLU를 사용한 경우의 가중치의 초깃값으로 'He 초기값'을 이용할 때의 각 층의 활성화값 분포](assets/img/Book/Deep-Learning/Weight Init He.png)
_활성화 함수로 ReLU를 사용한 경우의 가중치의 초깃값으로 'He 초기값'을 이용할 때의 각 층의 활성화값 분포_

**우수한 성능:**
- 표준편차 0.01: 기울기 소실 문제 발생
- Xavier: 점차 치우침 발생으로 기울기 소실 문제 유발
- **He 초기화**: ReLU와 함께 사용할 때 최적의 분포 유지

---

## 실제 성능 비교: MNIST 데이터셋

### 초기화 방법별 학습 성능

![MNIST 데이터셋으로 살펴본 '가중치의 초깃값'에 따른 비교](assets/img/Book/Deep-Learning/Weight Init Compare.png)
_MNIST 데이터셋으로 살펴본 '가중치의 초깃값'에 따른 비교_

**실험 조건:**
- 활성화 함수: ReLU
- 데이터셋: MNIST

**결과 분석:**
- **std=0.01**: 전혀 학습이 이루어지지 않음
  - 순전파에서 너무 작은 값이 흐름
  - 역전파 시 기울기도 작아져 가중치 갱신이 안 됨
- **Xavier와 He**: 순조로운 학습 진행
- **He 초기화**: ReLU와 함께 사용할 때 가장 빠른 학습 속도

---

## 배치 정규화: 초기화 의존성을 줄이는 혁신

### 배치 정규화의 아이디어

"각 층의 활성화값이 적당히 분포되도록 **강제**하면 어떨까?"

이런 아이디어에서 배치 정규화가 탄생했다.

### 배치 정규화의 장점

1. **학습 속도 개선**: 빠른 학습 진행
2. **초기값 독립성**: 초깃값에 크게 의존하지 않음
3. **과대적합 억제**: 드롭아웃 등의 필요성 감소

### 배치 정규화 알고리즘

![배치 정규화를 사용한 신경망의 예](assets/img/Book/Deep-Learning/Batch Normalization Example.png)
_배치 정규화를 사용한 신경망의 예_

#### 0단계: 평균과 분산

$$평균: \mu_B \leftarrow \frac{1}{m}\sum\limits_{i=1}^{m} x_i$$

$$분산: \sigma_B^2 \leftarrow \frac{1}{m}\sum\limits_{i=1}^{m}(x_t - \mu_B)^2$$

#### 1단계: 정규화

$$\hat{x_i} \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$

- $B = \{x_1, x_2, \cdots, x_m\}$: m개의 미니배치
- $\mu_B$: 배치 평균
- $\sigma_B^2$: 배치 분산
- $\epsilon$: 0으로 나누기 방지 상수

**목적**: 입력 데이터를 평균 0, 분산 1로 정규화

#### 2단계: 확대와 이동

$$y_i \leftarrow \gamma\hat{x_i} + \beta$$

- $\gamma$: 확대 매개변수 (초기값 1)
- $\beta$: 이동 매개변수 (초기값 0)
- 두 값은 학습을 통해 적합한 값으로 조정된다

### 배치 정규화의 계산 그래프

![배치 정규화의 계산 그래프](assets/img/Book/Deep-Learning/Batch Normalization Computatational Graph.png)
_배치 정규화의 계산 그래프_

배치 정규화도 미분 가능한 변환이므로 오차역전파법으로 학습할 수 있다.

---

## 배치 정규화의 효과

### 실험 결과

![배치 정규화의 효과: 배치 정규화가 학습 속도를 높인다.](assets/img/Book/Deep-Learning/Batch Normalization Compare.png)
_배치 정규화의 효과: 배치 정규화가 학습 속도를 높인다._

**관찰된 효과:**
- **거의 모든 경우에서 학습 속도 향상**
- **초기값 의존성 감소**: 잘못된 초기값에서도 학습 가능
- **안정적인 학습**: 배치 정규화 없이는 학습이 전혀 안 되던 경우도 해결

### 배치 정규화의 실용적 가치

**기존 문제:**
- 가중치 초기화에 민감함
- 깊은 네트워크에서 기울기 소실/폭발
- 학습 속도 저하

**배치 정규화 해결책:**
- 각 층의 입력 분포를 안정화
- 깊은 네트워크에서도 안정적 학습
- 더 큰 학습률 사용 가능

---

## 가이드라인

### 활성화 함수별 초기화 선택

#### Sigmoid/Tanh 사용 시
```python
# Xavier 초기화
w = np.random.randn(fan_in, fan_out) * np.sqrt(1.0 / fan_in)
```

#### ReLU 사용 시
```python
# He 초기화
w = np.random.randn(fan_in, fan_out) * np.sqrt(2.0 / fan_in)
```

### 배치 정규화 적용 위치

**일반적 구조:**
```
입력 → [선형변환 → 배치정규화 → 활성화함수] → 출력
```

**고려사항:**
- 배치 정규화는 보통 활성화 함수 전에 적용
- 일부 연구에서는 활성화 함수 후에 적용하기도 함
- 실험을 통해 최적 위치 결정

---

## 마무리

가중치 초기화와 배치 정규화는 신경망 학습의 안정성과 효율성을 크게 좌우한다.

**핵심 포인트:**
- **적절한 초기화**: 활성화 함수에 맞는 초기화 방법 선택
- **분포의 중요성**: 각 층의 활성화값이 고루 분포되어야 함
- **배치 정규화**: 초기화 의존성을 줄이고 학습을 안정화
- **실용적 선택**: ReLU + He 초기화 + 배치 정규화가 현재 표준

다음 단계에서는 과대적합 방지를 위한 드롭아웃, 가중치 감소 등의 정규화 기법들을 살펴보겠다.
