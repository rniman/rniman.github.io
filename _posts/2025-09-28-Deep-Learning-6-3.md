---
title: 밑바닥부터 시작하는 딥러닝 1 | 학습 관련 기술들 3
date: 2025-09-28 14:28:43 +0900
categories: [Study, Study\DeepLearning]
tags: [AI, Deep Learning, Python, Book Note]
author: "rniman"                            # for single entry
# or
# authors: [<author1_id>, <author2_id>]   # for multiple entries
description: "밑바닥부터 시작하는 딥러닝 1 Chapter 6 정리"
math: true
comments: false # 댓글 기능
---

## 책 정보 📖 

- 책 제목: 밑바닥부터 시작하는 딥러닝 1
- 글쓴이: 사이토 고키
- 옮긴이: 개앞맵시
- 출판사: 한빛미디어
- 발행일: 2025년 01월 24일 
- 챕터: Chapter 6. 학습 관련 기술들 

### 책소개
딥러닝 분야 부동의 베스트셀러!
머리로 이해하고 손으로 익히는 가장 쉬운 딥러닝 입문서
이 책은 딥러닝의 핵심 개념을 ‘밑바닥부터’ 구현해보며 기초를 한 걸음씩 탄탄하게 다질 수 있도록 도와주는 친절한 안내서입니다. 라이브러리나 프레임워크에 의존하지 않고 딥러닝의 기본 개념부터 이미지 인식에 활용되는 합성곱 신경망(CNN)까지 딥러닝의 원리를 체계적으로 설명합니다. 또한 복잡한 개념은 계산 그래프를 활용해 시각적으로 전달하여 누구나 쉽게 이해할 수 있습니다. 이 책은 딥러닝에 첫발을 내딛는 입문자는 물론이고 기초를 다시금 다지고 싶은 개발자와 연구자에게도 훌륭한 길잡이가 되어줄 것입니다.

* 출처 : [밑바닥부터 시작하는 딥러닝 1 - 교보문고](https://product.kyobobook.co.kr/detail/S000215599933)

### 주요 내용

- 정규화와 하이퍼파라미터 튜닝: 과대적합 방지와 성능 최적화

신경망이 복잡해질수록 **과대적합(Overfitting)** 문제가 심각해진다. 과대적합은 모델이 훈련 데이터에만 지나치게 적응되어 새로운 데이터에 대한 성능이 떨어지는 현상이다. 이를 해결하기 위한 정규화 기법들과 최적의 성능을 위한 하이퍼파라미터 튜닝 방법을 살펴보자.

---

## 과대적합 이해하기

### 과대적합이 발생하는 조건

과대적합은 주로 다음 경우에 발생한다:

1. **매개변수가 많고 표현력이 높은 모델**
2. **훈련 데이터가 적음**

### 과대적합 현상 관찰

![훈련 데이터(train)와 시험 데이터(test)의 에포크별 정확도 추이](assets/img/Book/Deep-Learning/Overfit with Less Learning Data.png)
_훈련 데이터(train)와 시험 데이터(test)의 에포크별 정확도 추이_

**관찰되는 현상:**
- 훈련 데이터 정확도: 100에포크 이후 거의 100% 달성
- 시험 데이터 정확도: 훈련 데이터와 큰 차이 발생
- **결론**: 모델이 훈련 데이터에만 적응해버린 상태

이는 모델이 일반화 능력을 잃고 훈련 데이터의 특성만 암기하게 된 것이다.

---

## 가중치 감소(Weight Decay): 전통적 정규화 기법

### 가중치 감소의 원리

과대적합은 가중치 매개변수의 값이 커서 발생하는 경우가 많다. 가중치 감소는 학습 과정에서 큰 가중치에 대해 큰 페널티를 부과하여 과대적합을 억제한다.

### 수학적 정의

**L2 정규화**: 모든 가중치의 제곱합을 손실함수에 추가한다.

$$\text{손실함수} = \text{원래 손실} + \frac{1}{2}\lambda W^2$$

- $\lambda$: 정규화 강도 조절 하이퍼파라미터
- $\lambda$가 클수록 큰 가중치에 대한 페널티가 커진다
- $\frac{1}{2}$: 미분 시 계산 편의를 위한 상수

### 기울기 계산

가중치의 기울기 계산 시:
$$\frac{\partial}{\partial W}\text{손실함수} = \text{원래 기울기} + \lambda W$$

### 가중치 감소의 효과

![가중치 감소를 이용한 훈련 데이터(train)과 시험 데이터(test)에 대한 정확도 추이](assets/img/Book/Deep-Learning/Overfit Weight Decay.png)
_가중치 감소를 이용한 훈련 데이터(train)과 시험 데이터(test)에 대한 정확도 추이_

**결과 분석 ($\lambda = 0.1$ 적용):**
- 훈련 데이터와 시험 데이터 간 차이가 어느 정도 감소
- 훈련 데이터 정확도가 100%에 도달하지 못함 (일반화 향상)
- 여전히 차이는 존재하지만 과대적합이 억제됨

---

## 드롭아웃(Dropout): 현대적 정규화 기법

### 드롭아웃의 아이디어

가중치 감소만으로는 복잡한 신경망의 과대적합을 완전히 해결하기 어렵다. 드롭아웃은 **뉴런을 임의로 삭제**하면서 학습하는 혁신적인 방법이다.

### 드롭아웃의 동작 원리

#### 훈련 시
- 은닉층의 뉴런을 무작위로 선택하여 삭제
- 삭제된 뉴런은 신호를 전달하지 않음
- 매번 다른 뉴런을 무작위로 삭제

#### 시험 시
- 모든 뉴런에 신호를 전달
- 각 뉴런의 출력에 (1 - 삭제 비율)을 곱해 조정

### 드롭아웃 구현

```python
class Dropout:
    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None

    def forward(self, x, train_flg=True):
        if train_flg:
            # 훈련 시: 무작위로 뉴런 삭제
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x * self.mask
        else:
            # 시험 시: 모든 뉴런 사용하되 비율 조정
            return x * (1.0 - self.dropout_ratio)

    def backward(self, dout):
        # 순전파에서 삭제된 뉴런은 역전파에서도 차단
        return dout * self.mask
```

### 드롭아웃의 핵심 메커니즘

**마스크 생성:**
- `np.random.rand(*x.shape) > self.dropout_ratio`
- 무작위 값이 dropout_ratio보다 큰 경우만 True
- True인 뉴런만 신호 전달

**역전파 처리:**
- ReLU와 동일한 방식
- 순전파에서 통과시킨 뉴런만 역전파에서도 통과

### 드롭아웃의 효과

![드롭아웃을 적용한 결과 (dropout_ratio = 0.15)](assets/img/Book/Deep-Learning/Overfit Dropout.png)
_드롭아웃을 적용한 결과 (dropout_ratio = 0.15)_

**놀라운 개선 효과:**
- 훈련 데이터와 시험 데이터 정확도 차이가 현저히 감소
- 훈련 데이터 정확도가 100%에 도달하지 않음 (적절한 일반화)
- 표현력을 유지하면서 과대적합을 효과적으로 억제

### 드롭아웃과 앙상블 학습

**앙상블 학습의 개념:**
- 개별적으로 학습시킨 여러 모델의 출력을 평균 내어 추론
- 일반적으로 단일 모델보다 성능이 우수

**드롭아웃의 앙상블 효과:**
- 매번 다른 뉴런을 삭제 = 매번 다른 모델을 학습
- 하나의 네트워크로 앙상블 효과 달성
- 계산 비용 증가 없이 성능 향상

---

## 하이퍼파라미터 최적화

### 검증 데이터의 필요성

#### 데이터 분할 전략

**기존 분할:**
- 훈련 데이터: 모델 학습용
- 시험 데이터: 최종 성능 평가용

**문제점:**
시험 데이터로 하이퍼파라미터를 조정하면 시험 데이터에 과대적합 발생

**해결책:**
- **검증 데이터(Validation Data)** 도입
- 훈련 데이터의 일부를 검증 데이터로 분리
- 하이퍼파라미터 최적화는 검증 데이터로 수행

#### 최종 데이터 분할

```
전체 데이터
├── 훈련 데이터 (60%)  → 모델 학습
├── 검증 데이터 (20%)  → 하이퍼파라미터 튜닝
└── 시험 데이터 (20%)  → 최종 성능 평가
```

### 하이퍼파라미터 최적화 전략

#### 기본 원칙

하이퍼파라미터 최적화의 핵심은 **'최적값'이 존재하는 범위를 조금씩 줄여나가는 것**이다.

#### 최적화 과정

**1단계: 범위 설정**
- 하이퍼파라미터 값의 대략적인 범위 설정
- 로그 스케일 사용 권장 (예: $10^{-6}$ ~ $10^{-2}$)

**2단계: 무작위 샘플링**
- 설정된 범위에서 하이퍼파라미터 값을 무작위로 추출
- 그리드 서치보다 무작위 샘플링이 더 효과적

**3단계: 성능 평가**
- 샘플링한 값으로 학습하고 검증 데이터로 정확도 평가
- 에포크는 작게 설정 (빠른 평가를 위해)

**4단계: 범위 축소**
- 1~3단계를 여러 번 반복 (100회 등)
- 결과를 바탕으로 하이퍼파라미터 범위를 좁힘

**5단계: 반복**
- 범위가 충분히 좁아질 때까지 반복
- 최종적으로 최적값 선택

### 하이퍼파라미터 최적화 구현 예시

#### 학습률과 가중치 감소 계수 탐색

```python
# 로그 스케일에서 무작위 샘플링
weight_decay = 10 ** np.random.uniform(-8, -4)  # 10^-8 ~ 10^-4
lr = 10 ** np.random.uniform(-6, -2)            # 10^-6 ~ 10^-2
```

#### 최적화 결과 분석

![실선은 검증 데이터에 대한 정확도, 점선은 훈련 데이터에 대한 정확도](assets/img/Book/Deep-Learning/Optimal Hyperparameter.png)
_실선은 검증 데이터에 대한 정확도, 점선은 훈련 데이터에 대한 정확도_

**분석 방법:**
1. 학습이 순조롭게 이뤄진 결과 식별
2. 해당 결과의 하이퍼파라미터 값 확인
3. 좋은 성능을 보이는 값들의 범위 파악

![Best Optimal Hyperparameter](assets/img/Book/Deep-Learning/Best Optimal Hyperparameter.png)

**다음 단계:**
- 관찰된 범위를 바탕으로 탐색 범위 축소
- 더 세밀한 탐색 수행
- 최종 하이퍼파라미터 값 선택

### 고급 최적화 기법

#### 베이즈 최적화

**기존 방법의 한계:**
- 수행자의 직관과 경험에 의존
- 비효율적인 탐색 가능성

**베이즈 최적화의 장점:**
- 베이즈 정리 기반의 수학적으로 엄밀한 접근
- 이전 실험 결과를 활용한 효율적인 탐색
- 불확실성을 고려한 지능적인 다음 탐색점 선택

**참고 자료:**
"Practical Bayesian Optimization of Machine Learning Algorithms" 논문 등

---

## 가이드라인

### 정규화 기법 선택

#### 가중치 감소 사용 시기
- 간단한 모델에서 기본적인 과대적합 억제
- 다른 정규화 기법과 함께 사용
- 일반적으로 $\lambda = 0.01$ ~ $0.1$ 범위에서 시작

#### 드롭아웃 사용 시기
- 복잡한 신경망에서 강력한 정규화 필요
- 완전연결층에서 주로 사용 (CNN의 합성곱층에서는 제한적)
- 일반적으로 dropout_ratio = 0.2 ~ 0.5

### 하이퍼파라미터 튜닝 팁

#### 효율적인 탐색 전략
1. **코스 투 파인(Coarse-to-Fine)**: 넓은 범위에서 시작해 점진적으로 축소
2. **로그 스케일**: 학습률, 정규화 계수 등은 로그 스케일로 탐색
3. **조기 종료**: 명백히 나쁜 결과는 빨리 중단
4. **병렬 실행**: 가능하면 여러 실험을 동시에 수행

#### 주요 하이퍼파라미터 우선순위
1. **학습률**: 가장 중요한 하이퍼파라미터
2. **배치 크기**: 메모리와 성능의 트레이드오프
3. **정규화 계수**: 과대적합 방지
4. **네트워크 구조**: 층수, 뉴런 수

---

## 종합 정리

신경망의 성능을 최대화하려면 다양한 기법을 체계적으로 적용해야 한다.

### 핵심 기술들

**최적화 기법:**
- SGD, 모멘텀, AdaGrad, Adam 등 다양한 옵티마이저

**초기화 기법:**
- Xavier 초기화 (Sigmoid/Tanh용)
- He 초기화 (ReLU용)

**정규화 기법:**
- 배치 정규화: 학습 안정화 및 가속화
- 가중치 감소: 전통적이고 안정적인 정규화
- 드롭아웃: 강력한 과대적합 방지

**하이퍼파라미터 최적화:**
- 체계적인 탐색 전략
- 검증 데이터 활용
- 베이즈 최적화 등 고급 기법

### 적용 순서

1. **기본 설정**: ReLU + He 초기화 + 배치 정규화
2. **옵티마이저**: Adam으로 시작
3. **정규화**: 과대적합 발생 시 드롭아웃 추가
4. **하이퍼파라미터**: 체계적 탐색으로 최적화

이러한 기법들을 잘 조합하면 안정적이고 높은 성능의 신경망을 구축할 수 있다.
